{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57fbb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soxr in /opt/homebrew/anaconda3/envs/iml-voice-recognition/lib/python3.9/site-packages (0.5.0.post1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/iml-voice-recognition/lib/python3.9/site-packages (from soxr) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install soxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ebbde91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU keras librosa tensorflow tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0833ee-7364-4c0f-821e-0542a95aaf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 12:18:27.070617: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-10-23 12:18:27.070647: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-10-23 12:18:27.070653: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-10-23 12:18:27.070675: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-23 12:18:27.070692: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "# basic algebra and system libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import gc\n",
    "# signal discrete fourier transform and loading\n",
    "import scipy.signal as signal\n",
    "import scipy.io.wavfile as wavfile\n",
    "import scipy.fft as fft\n",
    "# audio preprocessing, cross validation and others\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# construction of convolutional neural network\n",
    "import keras\n",
    "import keras.optimizers as optimizers\n",
    "import keras.layers as layers\n",
    "import keras.losses as losses\n",
    "import keras.metrics as metrics\n",
    "import keras.regularizers as regularizers\n",
    "# audio analysis\n",
    "import librosa\n",
    "import librosa.display\n",
    "# processing speedup using gpu\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "# allow gpu computation to exceed system memory for a certain amount\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce16b789-6622-4ffe-9544-898b91dd8ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f3554e-9114-4441-b5b8-4fd1069604ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "data_dir = os.path.join(\"data\", \"raw\")\n",
    "metadata['data_dir'] = data_dir\n",
    "metadata['path'] = []\n",
    "metadata['label'] = []\n",
    "class_1_speakers = ['f1', 'f7', 'f8', 'm3', 'm6', 'm8']\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            metadata['path'].append(file)\n",
    "            speaker_id = file.split('_')[0]\n",
    "            label = 1 if speaker_id in class_1_speakers else 0\n",
    "            metadata['label'].append(label)\n",
    "            \n",
    "            \n",
    "audios = [ wavfile.read(path.join(\"data\", \"raw\", audio_path)) for audio_path in metadata['path'] ]\n",
    "metadata['rate'] = [ sample_rate for sample_rate, _ in audios ]\n",
    "metadata['signal'] = [ wave for _, wave in audios ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50613181-efa5-4bf6-85d4-c709bec943a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class AudioPreprocessor(TransformerMixin):\n",
    "    def __init__(self, sr: int = 16000) -> None:\n",
    "        self.sr = sr\n",
    "\n",
    "    def fit(self, X, y = None, **params):\n",
    "        X_resampled = X.copy()\n",
    "        # first resample to target frequency 16000, as audio signals are storead as 16-bit signed integers, we need to normalize them  by dividing by 32768\n",
    "        X_resampled['signal'] = [ librosa.resample(signal / 32768, orig_sr = rate, target_sr = self.sr, res_type = 'soxr_qq') if rate != self.sr else (signal / 32768) for rate, signal in zip(X_resampled['rate'], X_resampled['signal']) ]\n",
    "        # expect X to be full dataset with signal column holding wave signal arrays\n",
    "        # learn fixed count of samples from trimmed wave signals\n",
    "        #trimmed slcices mean that we remove sections of the signal where the amplitude is below top_db\n",
    "        trimmed = [ librosa.effects.trim(audio, top_db = 20)[0] for audio in X_resampled['signal'] ]\n",
    "        # take 95% percentile to compute the chosen length for all recordings\n",
    "        self.samples = int(np.percentile([ audio.shape[0] - np.argmin((audio == 0)[::-1]) for audio in trimmed ], 95))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None, **params):\n",
    "        X_resampled = X.copy()\n",
    "        # first resample to target frequency 16000\n",
    "        X_resampled['signal'] = [ librosa.resample(signal / 32768, orig_sr = rate, target_sr = self.sr, res_type = 'soxr_qq') if rate != self.sr else (signal / 32768) for rate, signal in zip(X_resampled['rate'], X_resampled['signal']) ]\n",
    "        # trim and pad wave files\n",
    "        trimmed = [ librosa.effects.trim(audio, top_db = 20)[0] for audio in X_resampled['signal'] ]\n",
    "        waves = np.stack([ audio[:self.samples] if audio.shape[0] >= self.samples else np.pad(audio, (0, self.samples - audio.shape[0]), constant_values = (0, 0)) for audio in trimmed ], axis = 0)    \n",
    "        # result is wave signals with corresponding indexes\n",
    "        return waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f21c164-8d08-4bc2-8dc8-4c6c68eee5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m audio_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Create the first spectrogram\u001b[39;00m\n\u001b[1;32m     30\u001b[0m spectrogram1 \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmelspectrogram(\n\u001b[0;32m---> 31\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43maudio_id\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32768\u001b[39m,\n\u001b[1;32m     32\u001b[0m     sr \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrate\u001b[39m\u001b[38;5;124m'\u001b[39m][audio_id],\n\u001b[1;32m     33\u001b[0m     n_fft \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \n\u001b[1;32m     34\u001b[0m     hop_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     35\u001b[0m     n_mels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create the second spectrogram (can be a different audio file or manipulated version of the first)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m audio_id2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class MelSpectrogram(TransformerMixin):\n",
    "    # Extracts spectrogram with frequencies in Mel scale, thus Mel spectrogram\n",
    "    def __init__(self, sr: int = 16000) -> None:\n",
    "        self.sr = sr\n",
    "    \n",
    "    def fit(self, X, y = None, **params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None, **params):\n",
    "        # extraction of Mel spectrograms\n",
    "        spectrograms = np.array([ \n",
    "            librosa.feature.melspectrogram(\n",
    "                y = x, \n",
    "                sr = self.sr,\n",
    "                n_fft = 2048, \n",
    "                hop_length = 512,\n",
    "                n_mels = 128\n",
    "            ) \n",
    "            for x in X \n",
    "        ])\n",
    "        # one unitary dimension is added to make data format\n",
    "        # compatible with respect to keras input\n",
    "        return np.expand_dims(spectrograms, axis = -1)\n",
    "\n",
    "# example \"dirty\" spectrogram\n",
    "audio_id = 90\n",
    "\n",
    "# Create the first spectrogram\n",
    "spectrogram1 = librosa.feature.melspectrogram(\n",
    "    y = metadata['signal'][audio_id] / 32768,\n",
    "    sr = metadata['rate'][audio_id],\n",
    "    n_fft = 2048, \n",
    "    hop_length = 512,\n",
    "    n_mels = 128\n",
    ")\n",
    "\n",
    "# Create the second spectrogram (can be a different audio file or manipulated version of the first)\n",
    "audio_id2 = 11\n",
    "spectrogram2 = librosa.feature.melspectrogram(\n",
    "    y = metadata['signal'][audio_id2] / 32768,\n",
    "    sr = metadata['rate'][audio_id2],\n",
    "    n_fft = 2048, \n",
    "    hop_length = 512,\n",
    "    n_mels = 128\n",
    ")\n",
    "\n",
    "audio_id3 = 3\n",
    "spectrogram3 = librosa.feature.melspectrogram(\n",
    "    y = metadata['signal'][audio_id3] / 32768,\n",
    "    sr = metadata['rate'][audio_id3],\n",
    "    n_fft = 2048, \n",
    "    hop_length = 512,\n",
    "    n_mels = 128\n",
    ")\n",
    "\n",
    "# Create subplots for two spectrograms\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4))  # 1 row, 2 columns for side-by-side plots\n",
    "\n",
    "# Display first spectrogram\n",
    "img1 = librosa.display.specshow(\n",
    "    librosa.power_to_db(spectrogram1, ref = np.max),\n",
    "    sr = metadata['rate'][audio_id],\n",
    "    n_fft = 2048, \n",
    "    hop_length = 512,\n",
    "    x_axis = 'time',\n",
    "    y_axis = 'mel',\n",
    "    cmap = 'magma',\n",
    "    ax = axs[0]  # Plot on the first axis\n",
    ")\n",
    "axs[0].set_title(f\"Mel spectrogram [dB] of {os.path.basename(metadata['path'][audio_id])}\")\n",
    "axs[0].set_xlabel('Time [s]')\n",
    "axs[0].set_ylabel('Frequency [Hz]')\n",
    "fig.colorbar(img1, ax=axs[0], format='%+2.0f dB')\n",
    "\n",
    "# Display second spectrogram\n",
    "img2 = librosa.display.specshow(\n",
    "    librosa.power_to_db(spectrogram2, ref = np.max),\n",
    "    sr = metadata['rate'][audio_id2],\n",
    "    n_fft = 2048, \n",
    "    hop_length = 512,\n",
    "    x_axis = 'time',\n",
    "    y_axis = 'mel',\n",
    "    cmap = 'magma',\n",
    "    ax = axs[1]  # Plot on the second axis\n",
    ")\n",
    "axs[1].set_title(f\"Mel spectrogram [dB] of {os.path.basename(metadata['path'][audio_id2])}\")\n",
    "axs[1].set_xlabel('Time [s]')\n",
    "axs[1].set_ylabel('Frequency [Hz]')\n",
    "axs[1].tick_params(left=False)\n",
    "fig.colorbar(img2, ax=axs[1], format='%+2.0f dB')\n",
    "\n",
    "# Display second spectrogram\n",
    "img3 = librosa.display.specshow(\n",
    "    librosa.power_to_db(spectrogram3, ref = np.max),\n",
    "    sr = metadata['rate'][audio_id3],\n",
    "    n_fft = 2048, \n",
    "    hop_length = 512,\n",
    "    x_axis = 'time',\n",
    "    y_axis = 'mel',\n",
    "    cmap = 'magma',\n",
    "    ax = axs[2]  # Plot on the second axis\n",
    ")\n",
    "axs[2].set_title(f\"Mel spectrogram [dB] of {os.path.basename(metadata['path'][audio_id3])}\")\n",
    "axs[2].set_xlabel('Time [s]')\n",
    "axs[2].set_ylabel('Frequency [Hz]')\n",
    "axs[2].tick_params(left=False)\n",
    "fig.colorbar(img3, ax=axs[2], format='%+2.0f dB')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9d2968-1c09-4ebc-bfb8-2c6d63f137f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_convolutional_neural_network(input_shape, n_classes):\n",
    "    # Defining the model using TensorFlow/Keras\n",
    "    model = models.Sequential([\n",
    "        # Input shape of Mel-spectrogram (n_mels, n_frames)\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        # Add Gaussian noise to input data\n",
    "        layers.GaussianNoise(0.1),\n",
    "\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(16, (7, 7), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Second convolutional block\n",
    "        #layers.Conv2D(32, (5, 5), activation='relu'),\n",
    "        #layers.BatchNormalization(),\n",
    "        #layers.Dropout(0.25),\n",
    "        #layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Third convolutional block\n",
    "        #layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "        #layers.BatchNormalization(),\n",
    "        #layers.Dropout(0.25),\n",
    "        #layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Flatten the 2D data into 1D to prepare for the Dense layers\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Fully connected layer with 128 units\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        # Output layer with softmax activation for multi-class classification\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with Adam optimizer and sparse categorical cross-entropy loss\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abefeb0-ff89-4bd4-b3b8-eb179df058e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "class ConvolutionalNeuralNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_classes: int):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'n_classes': self.n_classes}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.n_classes = params['n_classes']\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y, **params):\n",
    "        # Get unique classes for class weighting\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "        # Create the CNN model using TensorFlow\n",
    "        self.model_ = create_convolutional_neural_network(\n",
    "            input_shape=X.shape[1:], \n",
    "            n_classes=self.n_classes\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        self.model_.fit(\n",
    "            X,\n",
    "            y,\n",
    "            batch_size=64,\n",
    "            class_weight=dict(zip(classes, compute_class_weight('balanced', classes=classes, y=y))),\n",
    "            epochs=25,\n",
    "            verbose=1,\n",
    "            callbacks=[tensorboard_callback]\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None, **params):\n",
    "        return np.argmax(self.model_.predict(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cf15a-7c32-4629-b739-7f4695121e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_score, recall_score\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "preprocessing = Pipeline([\n",
    "    ('audio_preprocessor', AudioPreprocessor(sr=16000)),\n",
    "    ('feature_extractor', MelSpectrogram(sr=16000))\n",
    "])\n",
    "\n",
    "# Label encoding\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(metadata['label'])\n",
    "X = preprocessing.fit_transform(metadata)\n",
    "\n",
    "# Split data into train and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the CNN model\n",
    "model = ConvolutionalNeuralNetwork(n_classes=len(encoder.classes_))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Compute the macro-average F1 score\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Output the metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Macro F1: {macro_f1:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546eb43-cc0a-4288-ac8a-5abc7de3344a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml-voice-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
